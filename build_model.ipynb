{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Others\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions: model fitting, data splitting, table merging ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(uhi_data, used_columns, split_ratio=0.3):\n",
    "    kept_data = uhi_data[used_columns]\n",
    "    X = kept_data.drop(columns=['UHI Index']).values\n",
    "    y = kept_data ['UHI Index'].values\n",
    "    if split_ratio == 0.0:\n",
    "        return X, None, y, None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio,random_state=123)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_perf(uhi_data, model, used_columns, split_ratio=0.3):\n",
    "    X_train, X_test, y_train, y_test = get_data(uhi_data, used_columns, split_ratio)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    sc = StandardScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "\n",
    "    y_train = np.round(y_train, 5)\n",
    "    y_train_processed = sc_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "    model.fit(X_train, y_train_processed)\n",
    "    \n",
    "    # Make predictions on the training data\n",
    "    insample_predictions = model.predict(X_train)\n",
    "    insample_predictions = sc_y.inverse_transform(insample_predictions.reshape(-1, 1)).flatten()\n",
    "    # calculate R-squared score for in-sample predictions\n",
    "    Y_train = y_train.tolist()\n",
    "    r2_train = r2_score(Y_train, insample_predictions)\n",
    "    if split_ratio == 0.0:\n",
    "        return sc, sc_y, model, r2_train, 0.0\n",
    "    \n",
    "    X_test = sc.transform(X_test)\n",
    "    # Make predictions on the test data\n",
    "    outsample_predictions = model.predict(X_test)\n",
    "    outsample_predictions = sc_y.inverse_transform(outsample_predictions.reshape(-1, 1)).flatten()\n",
    "    # calculate R-squared score for out-sample predictions\n",
    "    Y_test = y_test.tolist()\n",
    "    r2_test = r2_score(Y_test, outsample_predictions)\n",
    "    return sc, sc_y, model, r2_train, r2_test\n",
    "\n",
    "# Combine two datasets vertically (along columns) using pandas concat function.\n",
    "def combine_two_datasets(dataset1,dataset2):\n",
    "    data = pd.concat([dataset1, dataset2], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>datetime</th>\n",
       "      <th>UHI Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.909167</td>\n",
       "      <td>40.813107</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.030289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.909187</td>\n",
       "      <td>40.813045</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.030289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-73.909215</td>\n",
       "      <td>40.812978</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.023798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.909242</td>\n",
       "      <td>40.812908</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.023798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.909257</td>\n",
       "      <td>40.812845</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.021634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Longitude   Latitude          datetime  UHI Index\n",
       "0 -73.909167  40.813107  24-07-2021 15:53   1.030289\n",
       "1 -73.909187  40.813045  24-07-2021 15:53   1.030289\n",
       "2 -73.909215  40.812978  24-07-2021 15:53   1.023798\n",
       "3 -73.909242  40.812908  24-07-2021 15:53   1.023798\n",
       "4 -73.909257  40.812845  24-07-2021 15:53   1.021634"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_df = pd.read_csv(\"Training_data_uhi_index.csv\")\n",
    "ground_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (internal data) 1. Weather Data ###\n",
    "\n",
    "#### based on the distance of the interested point to the location of the weather station, assign the weather data to the interested point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_area_and_weather(in_data):\n",
    "    data = in_data.copy()\n",
    "    Bronx_coords = (-73.89352,40.87248)\n",
    "    Manhattan_coords = (-73.96449,40.76754)\n",
    "    # Calculate L2 (Euclidean) distance between two points\n",
    "    def l2_distance(point1, point2):\n",
    "        return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "    # For each point in data, calculate distance to Bronx and Manhattan coordinates\n",
    "    # and assign area based on which is closer\n",
    "    data['area'] = data.apply(\n",
    "        lambda row: 'Bronx' if l2_distance((row['Longitude'], row['Latitude']), Bronx_coords) < \n",
    "                            l2_distance((row['Longitude'], row['Latitude']), Manhattan_coords) \n",
    "                    else 'Manhattan', axis=1)\n",
    "\n",
    "    manhattan_weather = [ 26.73333333,48.54166667  , 2.96666667, 167.91666667, 426.08333333]\n",
    "    bronx_weather = [ 27.51666667 , 44.45833333 ,  3.125   ,   132.66666667, 454.91666667]\n",
    "\n",
    "    # Add weather features based on area\n",
    "    data['w1'] = data.apply(\n",
    "        lambda row: manhattan_weather[0] if row['area'] == 'Manhattan' else bronx_weather[0], axis=1)\n",
    "    data['w2'] = data.apply(\n",
    "        lambda row: manhattan_weather[1] if row['area'] == 'Manhattan' else bronx_weather[1], axis=1)\n",
    "    data['w3'] = data.apply(\n",
    "        lambda row: manhattan_weather[2] if row['area'] == 'Manhattan' else bronx_weather[2], axis=1)\n",
    "    data['w4'] = data.apply(\n",
    "        lambda row: manhattan_weather[3] if row['area'] == 'Manhattan' else bronx_weather[3], axis=1)\n",
    "    data['w5'] = data.apply(\n",
    "        lambda row: manhattan_weather[4] if row['area'] == 'Manhattan' else bronx_weather[4], axis=1)\n",
    "    data.drop(in_data.columns, axis=1, inplace=True)\n",
    "    data.drop(columns=['area'], inplace=True)\n",
    "    return data, ['w1','w2','w3','w4','w5']\n",
    "\n",
    "weather_data, weather_columns = define_area_and_weather(ground_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (internal data) 2. BUILDING FOOTPRINT within various buffer sizes ###\n",
    "\n",
    "for each interested point, we construct the buffer of it and caculate:\n",
    "1. the number of buildings within the buffer [we consider buffer size in [0.06, 0.07, 0.08, 0.09, 0.1]]\n",
    "2. the area of the building footprint within the buffer [we consider buffer size in [0.0005, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]]\n",
    "\n",
    "the computation is done under projection: EPSG:4326\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NUMBER OF BUILDINGS ###\n",
    "def get_number_buildings(lat, lon, buffer_size, gdf):\n",
    "    point = Point(lon, lat)\n",
    "    buffer = point.buffer(buffer_size)\n",
    "    return len(gdf[gdf['geometry'].intersects(buffer)])\n",
    "\n",
    "def process_row(index, row, buffer_sizes, gdf):\n",
    "    result = {}\n",
    "    for buffer_size in buffer_sizes:\n",
    "        num_buildings = get_number_buildings(row['Latitude'], row['Longitude'], buffer_size, gdf)\n",
    "        result[f'num_buildings_{buffer_size}'] = num_buildings\n",
    "    return index, result\n",
    "\n",
    "def construct_building_footprint(in_data, gdf, buffer_sizes):\n",
    "    data = in_data.copy()\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    results = [pool.apply_async(process_row, args=(index, row, buffer_sizes, gdf)) for index, row in data.iterrows()]\n",
    "\n",
    "    for res in tqdm(results):\n",
    "        index, result = res.get()\n",
    "        for key, value in result.items():\n",
    "            data.at[index, key] = value\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    data.drop(in_data.columns, axis=1, inplace=True)\n",
    "    return data, ['num_buildings_{}'.format(buffer_size) for buffer_size in buffer_sizes]\n",
    "\n",
    "### AREA OF BUILDING FOOTPRINT ###\n",
    "def building_area_within_buffer(lat, lon, buffer_size, gdf):\n",
    "    point = Point(lon, lat)\n",
    "    buffer = point.buffer(buffer_size)\n",
    "    overlap = gdf.intersection(buffer).area\n",
    "    return (overlap).sum()\n",
    "\n",
    "def process_row_area(index, row, buffer_sizes, gdf):\n",
    "    result = {}\n",
    "    for buffer_size in buffer_sizes:\n",
    "        area = building_area_within_buffer(row['Latitude'], row['Longitude'], buffer_size, gdf)\n",
    "        result[f'building_area_{buffer_size}'] = area\n",
    "    return index, result\n",
    "\n",
    "def construct_building_footprint_area(in_data, gdf, buffer_sizes):\n",
    "    data = in_data.copy()\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    results = [pool.apply_async(process_row_area, args=(index, row, buffer_sizes, gdf)) for index, row in data.iterrows()]\n",
    "\n",
    "    for res in tqdm(results):\n",
    "        index, result = res.get()\n",
    "        for key, value in result.items():\n",
    "            data.at[index, key] = value\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    data.drop(in_data.columns, axis=1, inplace=True)\n",
    "    return data, [f'building_area_{buffer_size}' for buffer_size in buffer_sizes]\n",
    "\n",
    "\n",
    "\n",
    "### AREA OF BUILDING FOOTPRINT ###\n",
    "if not os.path.exists('cached_tables/train_building_footprint_area.csv'):\n",
    "\n",
    "    kml_file = './raw_data/Building_Footprint.kml'\n",
    "    building_geometry = gpd.read_file(kml_file, driver='KML')\n",
    "    df = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    data, columns = construct_building_footprint_area(df, building_geometry, [0.0005,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1])\n",
    "    data.to_csv('cached_tables/train_building_footprint_area.csv', index=False)\n",
    "\n",
    "if not os.path.exists('cached_tables/val_building_footprint_area.csv'):\n",
    "    df = pd.read_csv('Submission_template.csv')\n",
    "    data, columns = construct_building_footprint_area(df, building_geometry, [0.0005,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1])\n",
    "    data.to_csv('cached_tables/val_building_footprint_area.csv', index=False)\n",
    "\n",
    "### NUMBER OF BUILDINGS ###\n",
    "if not os.path.exists('cached_tables/train_building_footprint.csv'):\n",
    "\n",
    "    kml_file = './raw_data/Building_Footprint.kml'\n",
    "    building_geometry = gpd.read_file(kml_file, driver='KML')\n",
    "    df = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    data, columns = construct_building_footprint(df, building_geometry, [0.06,0.07,0.08,0.09,0.1])\n",
    "    data.to_csv('cached_tables/train_building_footprint.csv', index=False)\n",
    "\n",
    "if not os.path.exists('cached_tables/val_building_footprint.csv'):\n",
    "    df = pd.read_csv('Submission_template.csv')\n",
    "    data, columns = construct_building_footprint(df, building_geometry, [0.06,0.07,0.08,0.09,0.1])\n",
    "    data.to_csv('cached_tables/val_building_footprint.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_footprint_area = pd.read_csv('cached_tables/train_building_footprint_area.csv')\n",
    "building_footprint_area_columns = ['building_area_0.0005', 'building_area_0.001', 'building_area_0.002', 'building_area_0.003', 'building_area_0.004', 'building_area_0.005', 'building_area_0.006', 'building_area_0.007', 'building_area_0.008', 'building_area_0.009', 'building_area_0.01', 'building_area_0.02', 'building_area_0.03', 'building_area_0.04', 'building_area_0.05', 'building_area_0.06', 'building_area_0.07', 'building_area_0.08', 'building_area_0.09', 'building_area_0.1' ]\n",
    "\n",
    "building_footprint = pd.read_csv('cached_tables/train_building_footprint.csv')\n",
    "building_footprint_columns = ['num_buildings_0.06', 'num_buildings_0.07', 'num_buildings_0.08', 'num_buildings_0.09', 'num_buildings_0.1' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (external data) 3. Building elevation within various buffer sizes ###\n",
    "\n",
    "#### Data page: https://data.cityofnewyork.us/City-Government/Building-Elevation-and-Subgrade-BES-/bsin-59hv/about_data\n",
    "\n",
    "#### Download: \n",
    "in terminal, run \n",
    "```bash\n",
    "wget -O raw_data/nyc_building_elevation.csv  https://data.cityofnewyork.us/api/views/bsin-59hv/rows.csv\n",
    "``` \n",
    "\n",
    "#### Data description:\n",
    "1. z_grade: The elevation of the building at it's lowest adjacent grade - the lowest point where the building touches the ground (feet).\n",
    "2. z_floor: The elevation of what is estimated to be the lowest actively used floor (feet)\n",
    "3. the_geom: Geometry column used for mapping (will convert to geometry column later)\n",
    "\n",
    "#### Feature derivation:\n",
    "for each interested point, we construct the buffer of it and caculate:\n",
    "1. average z_grade within the buffer\n",
    "2. average z_floor within the buffer\n",
    "\n",
    "we consider buffer size in [0.005, 0.01]\n",
    "\n",
    "the computation is done under projection: EPSG:4326\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_elevation_within_buffer(lat, lon, buffer_size, gdf):\n",
    "    point = Point(lon, lat)\n",
    "    buffer = point.buffer(buffer_size)\n",
    "    z_grade = gdf[gdf.within(buffer)]['z_grade'].mean()\n",
    "    z_floor = gdf[gdf.within(buffer)]['z_floor'].mean()\n",
    "    \n",
    "    return z_grade, z_floor\n",
    "\n",
    "def process_row_elevation(index, row, buffer_sizes, gdf):\n",
    "    result = {}\n",
    "    for buffer_size in buffer_sizes:\n",
    "        z_grade, z_floor = building_elevation_within_buffer(row['Latitude'], row['Longitude'], buffer_size, gdf)\n",
    "        result[f'building_z_grade_{buffer_size}'] = z_grade\n",
    "        result[f'building_z_floor_{buffer_size}'] = z_floor\n",
    "    return index, result\n",
    "\n",
    "def construct_building_elevation(in_data, gdf, buffer_sizes):\n",
    "    data = in_data.copy()\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    results = [pool.apply_async(process_row_elevation, args=(index, row, buffer_sizes, gdf)) for index, row in data.iterrows()]\n",
    "\n",
    "    for res in tqdm(results):\n",
    "        index, result = res.get()\n",
    "        for key, value in result.items():\n",
    "            data.at[index, key] = value\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    data.drop(in_data.columns, axis=1, inplace=True)\n",
    "    return data, [f'building_z_grade_{buffer_size}' for buffer_size in buffer_sizes] + [f'building_z_floor_{buffer_size}' for buffer_size in buffer_sizes]\n",
    "\n",
    "if not os.path.exists('cached_tables/train_building_elevation.csv'):\n",
    "    # run this in terminal wget -O raw_data/nyc_building_elevation.csv  https://data.cityofnewyork.us/api/views/bsin-59hv/rows.csv\n",
    "    elevation_file = './raw_data/nyc_building_elevation.csv'\n",
    "    building_elevation = gpd.read_file(elevation_file)\n",
    "    lower_left = (40.75, -74.01)\n",
    "    upper_right = (40.88, -73.86)\n",
    "\n",
    "    # cutoff the data to the area of interest\n",
    "    mask1 = building_elevation['longitude'].astype(float) <= -73.86\n",
    "    mask2 = building_elevation['longitude'].astype(float) >= -74.01\n",
    "    mask3 = building_elevation['latitude'].astype(float) <= 40.88\n",
    "    mask4 = building_elevation['latitude'].astype(float) >= 40.75\n",
    "    mask = mask1 & mask2 & mask3 & mask4\n",
    "    building_elevation = building_elevation[mask]\n",
    "    building_elevation = building_elevation[['longitude','latitude','z_grade', 'z_floor']]\n",
    "    building_elevation['geometry'] = gpd.points_from_xy(building_elevation['longitude'], building_elevation['latitude'])\n",
    "    building_elevation = gpd.GeoDataFrame(building_elevation, geometry='geometry')\n",
    "    building_elevation['z_grade'] = building_elevation['z_grade'].astype(float)\n",
    "    building_elevation['z_floor'] = building_elevation['z_floor'].astype(float)\n",
    "\n",
    "    df = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    data, columns = construct_building_elevation(df, building_elevation, [0.005, 0.01])\n",
    "    data.to_csv('cached_tables/train_building_elevation.csv', index=False)\n",
    "\n",
    "    df = pd.read_csv('Submission_template.csv')\n",
    "    data, columns = construct_building_elevation(df, building_elevation, [0.005, 0.01])\n",
    "    data.to_csv('cached_tables/val_building_elevation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_elevation_data = pd.read_csv('cached_tables/train_building_elevation.csv')\n",
    "building_elevation_columns = ['building_z_grade_0.005', 'building_z_floor_0.005', 'building_z_grade_0.01', 'building_z_floor_0.01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (external data) 4. Building height within various buffer sizes ###\n",
    "\n",
    "#### Data page: https://data.cityofnewyork.us/City-Government/Building-Footprints/5zhs-2jue/about_data\n",
    "\n",
    "#### Download: \n",
    "in terminal, run \n",
    "```bash\n",
    "wget -O raw_data/nyc_building_height.csv  https://data.cityofnewyork.us/api/views/5zhs-2jue/rows.csv\n",
    "``` \n",
    "\n",
    "#### Data description:\n",
    "1. HEIGHTROOF: The height of the roof above the ground elevation, not height above sea level.\n",
    "2. BIN: Building Identification Number. A number assigned by City Planning and used by Dept. of Buildings to reference information pertaining to an individual building. The first digit is a borough code (1 = Manhattan, 2 = The Bronx, 3 = Brooklyn, 4 = Queens, 5 = Staten Island). The remaining 6 digits are unique for buildings within that borough. In some cases where these 6 digits are all zeros (e.g. 1000000, 2000000, etc.) the BIN is unassigned or unknown.\n",
    "3. the_geom: Geometry column used for mapping (will convert to geometry column later)\n",
    "\n",
    "#### Feature derivation:\n",
    "for each interested point, we construct the buffer of it and caculate:\n",
    "1. Floor Area Ratio (FAR): the measurement of a building's floor area in relation to the size of the lot/parcel\n",
    "\n",
    "we consider buffer size in [500] meters\n",
    "\n",
    "the computation is done under projection: EPSG:3857\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_building_far(gdf, location, buffer_size, standard_floor_height=3):\n",
    "   \n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"GeoDataFrame has no CRS, please set the coordinate reference system first.\")\n",
    "    \n",
    "    gdf_projected = gdf.to_crs(epsg=3857)\n",
    "    \n",
    "    lon, lat = location\n",
    "    point = Point(lon, lat)\n",
    "    point_projected = gpd.GeoSeries([point], crs=\"EPSG:4326\").to_crs(epsg=3857).iloc[0]\n",
    "    \n",
    "    buffer_polygon = point_projected.buffer(buffer_size)\n",
    "    buffer_area = buffer_polygon.area \n",
    "    \n",
    "    buildings_in_buffer = gdf_projected[gdf_projected.intersects(buffer_polygon)].copy()\n",
    "\n",
    "    buildings_in_buffer['footprint_area'] = buildings_in_buffer.geometry.area\n",
    "    \n",
    "    # ------------------------------\n",
    "    # 1. FAR computation\n",
    "    buildings_in_buffer['estimated_floors'] = buildings_in_buffer['HEIGHTROOF'] / standard_floor_height\n",
    "    buildings_in_buffer['floor_area'] = buildings_in_buffer['footprint_area'] * buildings_in_buffer['estimated_floors']\n",
    "    total_floor_area = buildings_in_buffer['floor_area'].sum()\n",
    "    far = total_floor_area / buffer_area if buffer_area > 0 else None\n",
    "    \n",
    "    features = {\n",
    "        'FAR': far,\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def building_far_within_buffer(lat, lon, buffer_size, gdf):\n",
    "    location = (lon, lat)\n",
    "    result = extract_building_far(gdf, location, buffer_size)\n",
    "    return result\n",
    "\n",
    "def process_row_far(index, row, buffer_sizes, gdf):\n",
    "    result = {}\n",
    "    for buffer_size in buffer_sizes:\n",
    "        result = building_far_within_buffer(row['Latitude'], row['Longitude'], buffer_size, gdf)\n",
    "        result = {f'{k}@{buffer_size}':v for k,v in result.items()}\n",
    "    return index, result\n",
    "\n",
    "def construct_building_far(in_data, gdf, buffer_sizes):\n",
    "    data = in_data.copy()\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    pool = mp.Pool(16)\n",
    "    results = [pool.apply_async(process_row_far, args=(index, row, buffer_sizes, gdf)) for index, row in data.iterrows()]\n",
    "\n",
    "    for res in tqdm(results):\n",
    "        index, result = res.get()\n",
    "        for key, value in result.items():\n",
    "            data.at[index, key] = value\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    data.drop(in_data.columns, axis=1, inplace=True)\n",
    "    return data, []\n",
    "    \n",
    "if not os.path.exists('cached_tables/train_building_far.csv'): \n",
    "    # https://data.cityofnewyork.us/api/views/5zhs-2jue/rows.csv\n",
    "    building_height_file = './raw_data/nyc_building_height.csv'\n",
    "    building_height = gpd.read_file(building_height_file)\n",
    "    lower_left = (-74.01, 40.75)\n",
    "    upper_right = (-73.86, 40.88)\n",
    "\n",
    "    building_height['geometry'] = building_height['the_geom'].apply(wkt.loads)\n",
    "    building_height = gpd.GeoDataFrame(building_height, geometry='geometry')\n",
    "\n",
    "    building_height = building_height[['geometry', 'HEIGHTROOF']]\n",
    "    building_height['HEIGHTROOF'] = building_height['HEIGHTROOF'].astype(float)\n",
    "    building_height = gpd.GeoDataFrame(building_height, geometry='geometry')\n",
    "\n",
    "\n",
    "    tile_polygon = gpd.GeoDataFrame(geometry=[Polygon([lower_left, (lower_left[0], upper_right[1]), upper_right, (upper_right[0], lower_left[1]), lower_left])])\n",
    "    building_height = gpd.clip(building_height, tile_polygon)\n",
    "    building_height.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    mask1 = building_height.to_crs(epsg=3857).geometry.area>50\n",
    "    mask2 = building_height['HEIGHTROOF'] > 2\n",
    "    mask = mask1 & mask2\n",
    "    building_height = building_height[mask]\n",
    "\n",
    "    df = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    data, columns = construct_building_far(df, building_height, [500])\n",
    "    data.to_csv('data/train_building_far.csv', index=False)\n",
    "\n",
    "    df = pd.read_csv('Submission_template.csv')\n",
    "    data, columns = construct_building_far(df, building_height, [500]) # 500,\n",
    "    data.to_csv('data/val_building_far.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_far_data = pd.read_csv('cached_tables/train_building_far.csv')\n",
    "building_far_columns = ['FAR@500']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (external data) 5. Building energy consumption within various buffer sizes ###\n",
    "\n",
    "#### Data page: https://energy.cusp.nyu.edu/#/\n",
    "\n",
    "#### Download: \n",
    "in terminal, run \n",
    "```bash\n",
    "wget -O raw_data/nyc_building_energy.csv 'https://uil.carto.com/api/v2/sql?format=csv&filename=filtered_evt&q=SELECT%20*%20FROM%20table_2021_disclosure_ll84_viz_11_2022%20WHERE%20(year%20%3E=%201900%20AND%20year%20%3C=%202022)%20AND%20(gfa%20%3E=%2010000%20AND%20gfa%20%3C=%20500035)%20%20AND%20(eui%20%3E=%2050%20AND%20eui%20%3C=%20350)'\n",
    "``` \n",
    "\n",
    "#### Data description:\n",
    "1. Weather Normalized Source Energy Use Intensity (EUI): Total amount of the energy from all the raw fuel required to operate a property, including losses that take place during generation, transmission, and distribution normalized by the building gross square footage and expressed in kBtu per gross square foot (kBtu/ft2) of the property normalized for weather. Weather normalization facilitates comparison between different parts of the country and corrects for year-to-year differences in weather. Weather Normalized Source EUI values are the result of self-reported entries\n",
    "2. Water Use Intensity (WUI): The annual consumption of water in gallons per gross square foot (gal/ft2) of the property.\n",
    "3. Greenhouse Gas Intensity (GHG): The total direct and indirect greenhouse gases emitted due to energy used by the property per gross square foot of the property, reported in kilograms of carbon dioxide equivalent per square foot (kgCO2e/ft2). The carbon coefficient is based on New York City's EPA Emissions & Generation Resource Integrated Database (eGRID) sub-region.\n",
    "4. Borough, Block, Lot Number (BBL): It is a 10-digit identifier assigned by NYC Department of Finance for each property in New York City and described by a set of 3 numbers: borough code followed by the tax block and the tax lot. BBLs are used by many city agencies to identify real estate for taxes, zoning, construction, and other purposes.\n",
    "5. Gross Floor Area: Gross square footage of the property, per Department of Finance records.\n",
    "6. ENERGY STAR® Score: A 1-to-100 percentile ranking for specified building types, as calculated by Portfolio Manager, with 100 being the best score and 50 the median. It compares the energy performance of a building against the national Commercial Buildings Energy Consumption Survey (CBECS) database, and independent industry surveys for that building type. This rating is normalized for weather and building attributes in order to obtain a measure of efficiency.\n",
    "7. Building Energy Efficiency Rating: Building Energy Efficiency Rating is an A-D letter grade and a corresponding label based on the ENERGY STAR® Score that are required to be displayed near the entrance of certain buildings over 25,000 square feet in an effort to publicly disclose energy benchmarking information and to give New Yorkers a snapshot of the building's energy performance relative to its peers.\n",
    "8. Property Type: The self-reported property type, as selected from the property type options available in Portfolio Manager. This is not necessarily consistent with the property type designation in Department of Finance records. The building types listed on the main page are the top 14 categories with Other containing all remaining categories reported in the disclosure data that meet the criteria.\n",
    "\n",
    "\n",
    "#### Data preprocessing:\n",
    "We have the BIN (Building Identification Number) in each row but it does not come with the geometry. Thus we utilize the building_height data to merge the geometry to the building_energy data, since the building_height data has the BIN column.\n",
    "\n",
    "#### Feature derivation:\n",
    "for each interested point, we construct the buffer of it and caculate:\n",
    "1. EUI: the average EUI within the buffer\n",
    "\n",
    "we consider buffer size in [0.005, 0.01, 0.15], we found other columns are not useful for our model\n",
    "\n",
    "the computation is done under projection: EPSG:4326\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def building_energy_within_buffer(lat, lon, buffer_size, gdf):\n",
    "    point = Point(lon, lat)\n",
    "    buffer = point.buffer(buffer_size)\n",
    "    mask = gdf.intersects(buffer)\n",
    "    eui = gdf[mask]['eui'].mean()\n",
    "    return eui\n",
    "\n",
    "def process_row_energy(index, row, buffer_sizes, gdf):\n",
    "    result = {}\n",
    "    for buffer_size in buffer_sizes:\n",
    "        eui = building_energy_within_buffer(row['Latitude'], row['Longitude'], buffer_size, gdf)\n",
    "        result[f'building_eui_{buffer_size}'] = eui\n",
    "    return index, result\n",
    "\n",
    "def construct_building_energy(in_data, gdf, buffer_sizes):\n",
    "    data = in_data.copy()\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    results = [pool.apply_async(process_row_energy, args=(index, row, buffer_sizes, gdf)) for index, row in data.iterrows()]\n",
    "\n",
    "    for res in tqdm(results):\n",
    "        index, result = res.get()\n",
    "        for key, value in result.items():\n",
    "            data.at[index, key] = value\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    data.drop(in_data.columns, axis=1, inplace=True)\n",
    "    return data, [f'building_energy_{buffer_size}' for buffer_size in buffer_sizes]\n",
    "\n",
    "\n",
    "if not os.path.exists('cached_tables/train_building_energy.csv'):\n",
    "\n",
    "    building_energy = pd.read_csv('./raw_data/nyc_building_energy.csv')\n",
    "\n",
    "    building_height_file = './raw_data/nyc_building_height.csv'\n",
    "    building_height = gpd.read_file(building_height_file)\n",
    "    building_height['geometry'] = building_height['the_geom'].apply(wkt.loads)\n",
    "    building_height = gpd.GeoDataFrame(building_height, geometry='geometry')\n",
    "    building_height = building_height[['geometry', 'BIN']]\n",
    "    building_height['bin'] = building_height['BIN']\n",
    "\n",
    "    building_energy = building_energy.merge(building_height, on='bin', how='left')\n",
    "    building_energy = building_energy[['geometry','energy', 'eui', 'wui', 'ess', 'gfa', 'ghg', 'value']]\n",
    "    building_energy.dropna(inplace=True)\n",
    "    building_energy = gpd.GeoDataFrame(building_energy, geometry='geometry')\n",
    "\n",
    "    df = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    data, columns = construct_building_energy(df, building_energy, [0.005,0.01,0.15])\n",
    "    data.to_csv('cached_tables/train_building_energy.csv', index=False)\n",
    "\n",
    "    df = pd.read_csv('Submission_template.csv')\n",
    "    data, columns = construct_building_energy(df, building_energy, [0.005,0.01,0.15])\n",
    "    data.to_csv('cached_tables/val_building_energy.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_energy_data = pd.read_csv('cached_tables/train_building_energy.csv')\n",
    "building_energy_columns = ['building_eui_0.005', 'building_eui_0.01', 'building_eui_0.15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (external data) 6. Population data in NYC ###\n",
    "\n",
    "#### Data page: https://www.kaggle.com/datasets/muonneutrino/new-york-city-census-data\n",
    "\n",
    "#### Download: \n",
    "You need to download the data from the link above and put it in the raw_data folder, there are two files:\n",
    "1. nyc_census_tracts.csv\n",
    "2. census_block_loc.csv\n",
    "\n",
    "#### Data description of nyc_census_tracts.csv:\n",
    "1. CensusTract: Census tract code\n",
    "2. TotalPop: Population of the census tract\n",
    "\n",
    "#### Data description of census_block_loc.csv:\n",
    "1. BlockCode: Block code\n",
    "2. Latitude: Latitude of the block\n",
    "3. Longitude: Longitude of the block\n",
    "\n",
    "#### Data preprocessing:\n",
    "Note that BlockCode = '{CensusTract}{region id}', we we can merge the two table by merging on CensusTract and prefixing BlockCode. The final table has the following columns of interest:\n",
    "1. TotalPop: Population of the census tract\n",
    "2. Latitude: Latitude of the block\n",
    "3. Longitude: Longitude of the block\n",
    "\n",
    "There are other statistics in the table, which are not used in this project.\n",
    "\n",
    "#### Feature derivation:\n",
    "for each interested point, we find the cloese data point in the population and assign the TotalPop to the interested point:\n",
    "1. TotalPop\n",
    "\n",
    "the computation is done under projection: EPSG:4326\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_closest_location(block_loc, row, coords):\n",
    "    ground_location = np.array([row['Latitude'], row['Longitude']])\n",
    "    # print(coords.shape)\n",
    "    distances = ((coords - ground_location)**2).mean(axis=1)\n",
    "    closest_index = np.argmin(distances)\n",
    "    return block_loc.iloc[closest_index]\n",
    "\n",
    "if not os.path.exists('cached_tables/train_pop_data.csv'):\n",
    "\n",
    "    block_loc = pd.read_csv('raw_data/census_block_loc.csv')\n",
    "    pop_nyc = pd.read_csv('raw_data/nyc_census_tracts.csv')\n",
    "\n",
    "    block_loc['BlockCode'] = block_loc['BlockCode'].astype(str).str[:11]\n",
    "\n",
    "    #rename block code to census tract\n",
    "    block_loc.rename(columns={'BlockCode': 'CensusTract'}, inplace=True)\n",
    "    pop_nyc['CensusTract'] = pop_nyc['CensusTract'].astype(str)\n",
    "\n",
    "    #merge  block_loc and pop_nyc by CensusTract\n",
    "    block_loc = pd.merge(block_loc, pop_nyc, on='CensusTract', how='left')\n",
    "    block_loc.drop(columns=['County_x','State','County_y','Borough','CensusTract'], inplace=True)\n",
    "\n",
    "    block_loc = block_loc[block_loc['TotalPop'].notna()]\n",
    "    block_loc = block_loc[block_loc['TotalPop']!=0]\n",
    "\n",
    "    block_loc.to_csv('raw_data/nyc_pop.csv', index=False)\n",
    "\n",
    "    lats = block_loc['Latitude'].astype(float)\n",
    "    longs = block_loc['Longitude'].astype(float)\n",
    "\n",
    "    lats = np.array(lats)\n",
    "    longs = np.array(longs)\n",
    "    coords = np.column_stack((lats, longs))\n",
    "\n",
    "    data = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    data = data.apply(lambda row: find_closest_location(block_loc, row, coords), axis=1)\n",
    "    data.to_csv('cached_tables/train_pop_data.csv', index=False)\n",
    "\n",
    "\n",
    "    data = pd.read_csv('Submission_template.csv')\n",
    "    data = data.apply(lambda row: find_closest_location(block_loc, row, coords), axis=1)\n",
    "    data.to_csv('cached_tables/val_pop_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data = pd.read_csv('cached_tables/train_pop_data.csv')\n",
    "pop_columns = ['TotalPop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (external data) 7. Mapping Inequality Data in NYC ###\n",
    "\n",
    "#### Data page: https://dsl.richmond.edu/panorama/redlining/data/NY-Manhattan/ and https://dsl.richmond.edu/panorama/redlining/data/NY-Bronx\n",
    "\n",
    "#### Download: \n",
    "in terminal, run \n",
    "```bash\n",
    "wget -O raw_data/bronx.json  'https://dsl.richmond.edu/panorama/redlining/static/citiesData/NYBronx1938/geojson.json'\n",
    "wget -O raw_data/manhattan.json  'https://dsl.richmond.edu/panorama/redlining/static/citiesData/NYManhattan1937/geojson.json'\n",
    "``` \n",
    "\n",
    "#### Data description\n",
    "1. category_id: measure the income grade of the region\n",
    "2. area: the area of the region\n",
    "3. residential: category of the region\n",
    "4. commercial: category of the region\n",
    "5. industrial: category of the region\n",
    "\n",
    "#### Feature derivation:\n",
    "for each interested point, we find the region that the point is in and assign the following features to the interested point:\n",
    "1. category_id: is consider ordinal data\n",
    "2. area: is consider numerical data\n",
    "3. residential: is consider binary data\n",
    "4. commercial: is consider binary data\n",
    "5. industrial: is consider binary data\n",
    "\n",
    "the computation is done under projection: EPSG:4326\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cached_tables/train_inequality.csv'):\n",
    "\n",
    "    bronx = gpd.read_file('raw_data/bronx.json')\n",
    "    manhattan = gpd.read_file('raw_data/manhattan.json')\n",
    "\n",
    "    ineq_df = pd.concat([bronx, manhattan], ignore_index=True)\n",
    "    ineq_df = ineq_df[['geometry','category_id','area','residential','commercial','industrial']]\n",
    "    ineq_df['residential'] = ineq_df['residential'].astype(int)\n",
    "    ineq_df['commercial'] = ineq_df['commercial'].astype(int)\n",
    "    ineq_df['industrial'] = ineq_df['industrial'].astype(int)\n",
    "    ineq_df.loc[ineq_df['category_id'] == 276, 'category_id'] = 5\n",
    "    ineq_df.loc[ineq_df['category_id'] == 202, 'category_id'] = 6\n",
    "    ineq_df.loc[ineq_df['category_id'] == 16, 'category_id'] = 7\n",
    "    ineq_df.loc[ineq_df['category_id'] == 205, 'category_id'] = 8\n",
    "\n",
    "\n",
    "    def get_category_id(row):\n",
    "        matching_categories = ineq_df[ineq_df.geometry.contains(Point(row['Longitude'], row['Latitude']))]['category_id'].values\n",
    "        return matching_categories[0] if len(matching_categories) > 0 else None\n",
    "\n",
    "    def get_area(row):\n",
    "        matching_categories = ineq_df[ineq_df.geometry.contains(Point(row['Longitude'], row['Latitude']))]['area'].values\n",
    "        return matching_categories[0] if len(matching_categories) > 0 else None\n",
    "\n",
    "    def get_residential(row):\n",
    "        matching_categories = ineq_df[ineq_df.geometry.contains(Point(row['Longitude'], row['Latitude']))]['residential'].values\n",
    "        return matching_categories[0] if len(matching_categories) > 0 else None\n",
    "\n",
    "    def get_commercial(row):\n",
    "        matching_categories = ineq_df[ineq_df.geometry.contains(Point(row['Longitude'], row['Latitude']))]['commercial'].values\n",
    "        return matching_categories[0] if len(matching_categories) > 0 else None\n",
    "\n",
    "    def get_industrial(row):\n",
    "        matching_categories = ineq_df[ineq_df.geometry.contains(Point(row['Longitude'], row['Latitude']))]['industrial'].values\n",
    "        return matching_categories[0] if len(matching_categories) > 0 else None\n",
    "\n",
    "    data = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    data['area'] = data.apply(get_area, axis=1)\n",
    "    data['residential'] = data.apply(get_residential, axis=1)\n",
    "    data['commercial'] = data.apply(get_commercial, axis=1)\n",
    "    data['industrial'] = data.apply(get_industrial, axis=1)\n",
    "    data['category_id'] = data.apply(get_category_id, axis=1)\n",
    "    data['category_id'].fillna(9, inplace=True)\n",
    "    data['area'].fillna(data['area'].mean(),inplace=True)\n",
    "    data['residential'].fillna(1, inplace=True)\n",
    "    data['commercial'].fillna(0, inplace=True)\n",
    "    data['industrial'].fillna(0, inplace=True)\n",
    "    data = data[['category_id','area','residential','commercial','industrial']]\n",
    "    data.to_csv('cached_tables/train_inequality.csv',index=False)\n",
    "\n",
    "\n",
    "    data = pd.read_csv('Submission_template.csv')\n",
    "    data['category_id'] = data.apply(get_category_id, axis=1)\n",
    "    data['area'] = data.apply(get_area, axis=1)\n",
    "    data['residential'] = data.apply(get_residential, axis=1)\n",
    "    data['commercial'] = data.apply(get_commercial, axis=1)\n",
    "    data['industrial'] = data.apply(get_industrial, axis=1)\n",
    "    data['category_id'].fillna(9, inplace=True)\n",
    "    data['area'].fillna(data['area'].mean(),inplace=True)\n",
    "    data['residential'].fillna(1,inplace=True)\n",
    "    data['commercial'].fillna(0, inplace=True)\n",
    "    data['industrial'].fillna(0, inplace=True)\n",
    "    data = data[['category_id','area','residential','commercial','industrial']]\n",
    "    data.to_csv('cached_tables/val_inequality.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inequality_data = pd.read_csv('cached_tables/train_inequality.csv')\n",
    "inequality_columns = ['category_id','area','residential','commercial','industrial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (external data) 8. Commute Data in NYC ###\n",
    "\n",
    "#### Data page: https://a816-dohbesp.nyc.gov/IndicatorPublic/data-explorer/walking-driving-and-cycling/?id=2415#display=summary\n",
    "\n",
    "#### Download: \n",
    "In https://a816-dohbesp.nyc.gov/IndicatorPublic/data-explorer/walking-driving-and-cycling/?id=2415#display=summary, download the data by clicking the \"Download Data\" button. Save it as raw_data/nyc_commute.csv.\n",
    "\n",
    "In https://github.com/nychealth/EHDP-data/blob/production/geography/GeoLookup.csv, download the table and save it as raw_data/GeoLookup.csv.\n",
    "\n",
    "#### Data description of nyc_commute.csv:\n",
    "We have 'Bicycle (number)','Car, truck, or van (number)','Public transportation (number)','Walked (number)', 'Bicycle (percent)', 'Car, truck, or van (percent)', 'Public transportation (percent)', 'Walked (percent)' for each GeoID in NYC\n",
    "\n",
    "\n",
    "#### Data description of GeoLookup.csv:\n",
    "Provides the longitude and latitude of each GeoID in nyc_commute.csv\n",
    "\n",
    "#### Feature derivation:\n",
    "for each interested point, we find the GeoID that the point is cloest to and assign the following features to the interested point:\n",
    "1. Bicycle (number)\n",
    "2. Car, truck, or van (number)\n",
    "3. Public transportation (number)\n",
    "4. Walked (number)\n",
    "5. Bicycle (percent)\n",
    "6. Car, truck, or van (percent)\n",
    "7. Public transportation (percent)\n",
    "8. Walked (percent)\n",
    "\n",
    "the computation is done under projection: EPSG:4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cached_tables/train_commute.csv'):\n",
    "\n",
    "    commute = gpd.read_file('raw_data/nyc_commute.csv')\n",
    "    commute = commute[commute['TimePeriod']=='2017-21']\n",
    "    geolookup = pd.read_csv('raw_data/GeoLookup.csv')\n",
    "\n",
    "    geolookup['GeoID'] = geolookup['GeoID'].astype(int)\n",
    "    commute['GeoID'] = commute['GeoID'].astype(int)\n",
    "    commute_geoid = commute.merge(geolookup, on='GeoID', how='left')\n",
    "    commute_geoid = commute_geoid[['Lat','Long','Bicycle (number)','Car, truck, or van (number)','Public transportation (number)','Walked (number)', \n",
    "                                'Bicycle (percent)', 'Car, truck, or van (percent)', 'Public transportation (percent)', 'Walked (percent)']]\n",
    "\n",
    "    commute_geoid['Long'] = commute_geoid['Long'].astype(float)\n",
    "    commute_geoid['Lat'] = commute_geoid['Lat'].astype(float)\n",
    "    commute_geoid['Bicycle (number)'] = commute_geoid['Bicycle (number)'].map(lambda x: int(x.replace(',', '')) if x is not None else None)\n",
    "    commute_geoid['Car, truck, or van (number)'] = commute_geoid['Car, truck, or van (number)'].map(lambda x: int(x.replace(',', '')) if x is not None else None)\n",
    "    commute_geoid['Public transportation (number)'] = commute_geoid['Public transportation (number)'].map(lambda x: int(x.replace(',', '')) if x is not None else None)\n",
    "    commute_geoid['Walked (number)'] = commute_geoid['Walked (number)'].map(lambda x: int(x.replace(',', '')) if x is not None else None)\n",
    "    commute_geoid['Bicycle (percent)'] = commute_geoid['Bicycle (percent)'].map(lambda x: float(x.replace('%', '')) if x is not None else None)\n",
    "    commute_geoid['Car, truck, or van (percent)'] = commute_geoid['Car, truck, or van (percent)'].map(lambda x: float(x.replace('%', '')) if x is not None else None)\n",
    "    commute_geoid['Public transportation (percent)'] = commute_geoid['Public transportation (percent)'].map(lambda x: float(x.replace('%', '')) if x is not None else None)\n",
    "    commute_geoid['Walked (percent)'] = commute_geoid['Walked (percent)'].map(lambda x: float(x.replace('%', '')) if x is not None else None)\n",
    "\n",
    "    def get_bicycle(row):\n",
    "        commute_geoid['distance'] = ((commute_geoid['Lat'] - row['Latitude'])**2 + (commute_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = commute_geoid.loc[commute_geoid['distance'].idxmin()]['Bicycle (number)']\n",
    "        return matching_categories\n",
    "\n",
    "    def get_car(row):\n",
    "        commute_geoid['distance'] = ((commute_geoid['Lat'] - row['Latitude'])**2 + (commute_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = commute_geoid.loc[commute_geoid['distance'].idxmin()]['Car, truck, or van (number)']\n",
    "        return matching_categories\n",
    "    def get_public_transportation(row):\n",
    "        commute_geoid['distance'] = ((commute_geoid['Lat'] - row['Latitude'])**2 + (commute_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = commute_geoid.loc[commute_geoid['distance'].idxmin()]['Public transportation (number)']\n",
    "        return matching_categories\n",
    "\n",
    "    def get_walked(row):\n",
    "        commute_geoid['distance'] = ((commute_geoid['Lat'] - row['Latitude'])**2 + (commute_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = commute_geoid.loc[commute_geoid['distance'].idxmin()]['Walked (number)']\n",
    "        return matching_categories\n",
    "\n",
    "    def get_bicycle_percent(row):\n",
    "        commute_geoid['distance'] = ((commute_geoid['Lat'] - row['Latitude'])**2 + (commute_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = commute_geoid.loc[commute_geoid['distance'].idxmin()]['Bicycle (percent)']\n",
    "        return matching_categories\n",
    "\n",
    "    def get_car_percent(row):\n",
    "        commute_geoid['distance'] = ((commute_geoid['Lat'] - row['Latitude'])**2 + (commute_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = commute_geoid.loc[commute_geoid['distance'].idxmin()]['Car, truck, or van (percent)']\n",
    "        return matching_categories\n",
    "\n",
    "    def get_public_transportation_percent(row):\n",
    "        commute_geoid['distance'] = ((commute_geoid['Lat'] - row['Latitude'])**2 + (commute_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = commute_geoid.loc[commute_geoid['distance'].idxmin()]['Public transportation (percent)']\n",
    "        return matching_categories\n",
    "\n",
    "    def get_walked_percent(row):    \n",
    "        commute_geoid['distance'] = ((commute_geoid['Lat'] - row['Latitude'])**2 + (commute_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = commute_geoid.loc[commute_geoid['distance'].idxmin()]['Walked (percent)']\n",
    "        return matching_categories\n",
    "\n",
    "\n",
    "    data = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    data['bicycle'] = data.apply(get_bicycle, axis=1)\n",
    "    data['car'] = data.apply(get_car, axis=1)\n",
    "    data['public_transportation'] = data.apply(get_public_transportation, axis=1)\n",
    "    data['walked'] = data.apply(get_walked, axis=1)\n",
    "    data['bicycle_percent'] = data.apply(get_bicycle_percent, axis=1)\n",
    "    data['car_percent'] = data.apply(get_car_percent, axis=1)\n",
    "    data['public_transportation_percent'] = data.apply(get_public_transportation_percent, axis=1)\n",
    "    data['walked_percent'] = data.apply(get_walked_percent, axis=1)\n",
    "    data = data[['bicycle','car','public_transportation','walked', 'bicycle_percent', 'car_percent', 'public_transportation_percent', 'walked_percent']]\n",
    "    data.to_csv('cached_tables/train_commute.csv',index=False)\n",
    "\n",
    "\n",
    "    data = pd.read_csv('Submission_template.csv')\n",
    "    data['bicycle'] = data.apply(get_bicycle, axis=1)\n",
    "    data['car'] = data.apply(get_car, axis=1)\n",
    "    data['public_transportation'] = data.apply(get_public_transportation, axis=1)\n",
    "    data['walked'] = data.apply(get_walked, axis=1)\n",
    "    data['bicycle_percent'] = data.apply(get_bicycle_percent, axis=1)\n",
    "    data['car_percent'] = data.apply(get_car_percent, axis=1)\n",
    "    data['public_transportation_percent'] = data.apply(get_public_transportation_percent, axis=1)\n",
    "    data['walked_percent'] = data.apply(get_walked_percent, axis=1)\n",
    "    data = data[['bicycle','car','public_transportation','walked', 'bicycle_percent', 'car_percent', 'public_transportation_percent', 'walked_percent']]\n",
    "    data.to_csv('cached_tables/val_commute.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "commute_data = pd.read_csv('cached_tables/train_commute.csv')\n",
    "commute_columns = ['public_transportation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (external data) 9.Air Quality in NYC ###\n",
    "\n",
    "#### Data page: https://a816-dohbesp.nyc.gov/IndicatorPublic/data-explorer/air-quality/?id=2023#display=summary \n",
    "\n",
    "#### Download: \n",
    "In https://a816-dohbesp.nyc.gov/IndicatorPublic/data-explorer/air-quality/?id=2023#display=summary, download the data by clicking the \"Download Data\" button. Save it as raw_data/nyc_pm2.5.csv.\n",
    "\n",
    "In https://github.com/nychealth/EHDP-data/blob/production/geography/GeoLookup.csv, download the table and save it as raw_data/GeoLookup.csv. (same as the GeoLookup.csv in the Commute Data section)\n",
    "\n",
    "#### Data description of nyc_pm2.5.csv:\n",
    "We have'10th percentile mcg/m3','90th percentile mcg/m3','Mean mcg/m3' for each GeoID in NYC\n",
    "\n",
    "\n",
    "#### Data description of GeoLookup.csv:\n",
    "Provides the longitude and latitude of each GeoID in nyc_pm2.5.csv\n",
    "\n",
    "#### Feature derivation:\n",
    "for each interested point, we find the GeoID that the point is cloest to and assign the following features to the interested point:\n",
    "1. 10th percentile mcg/m3\n",
    "2. 90th percentile mcg/m3\n",
    "3. Mean mcg/m3\n",
    "\n",
    "the computation is done under projection: EPSG:4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cached_tables/train_air_quality.csv'):\n",
    "\n",
    "    pm25 = gpd.read_file('./raw_data/nyc_pm2.5.csv')\n",
    "    pm25 = pm25[pm25['TimePeriod']=='Summer 2023']\n",
    "\n",
    "    geolookup = pd.read_csv('./raw_data/GeoLookup.csv')\n",
    "\n",
    "    pm25['GeoID'] = pm25['GeoID'].astype(int)\n",
    "\n",
    "    pm25_geoid = pm25.merge(geolookup, on='GeoID', how='left')\n",
    "    pm25_geoid = pm25_geoid[['Lat','Long','10th percentile mcg/m3','90th percentile mcg/m3','Mean mcg/m3']]\n",
    "    pm25_geoid.rename(columns={'10th percentile mcg/m3':'10th pm25','90th percentile mcg/m3':'90th pm25','Mean mcg/m3':'Mean pm25'}, inplace=True)\n",
    "    pm25_geoid.dropna(inplace=True)\n",
    "\n",
    "    def get_pm25(row):\n",
    "        pm25_geoid['distance'] = ((pm25_geoid['Lat'] - row['Latitude'])**2 + (pm25_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = pm25_geoid.loc[pm25_geoid['distance'].idxmin()]['Mean pm25']\n",
    "        return matching_categories\n",
    "\n",
    "    def get_pm25_10th(row):\n",
    "        pm25_geoid['distance'] = ((pm25_geoid['Lat'] - row['Latitude'])**2 + (pm25_geoid['Long'] - row['Longitude'])**2)**0.5\n",
    "        matching_categories = pm25_geoid.loc[pm25_geoid['distance'].idxmin()]['10th pm25']\n",
    "        return matching_categories\n",
    "\n",
    "\n",
    "    data = pd.read_csv('Training_data_uhi_index.csv')\n",
    "\n",
    "    data['pm25'] = data.apply(get_pm25, axis=1)\n",
    "    data['pm25_10th'] = data.apply(get_pm25_10th, axis=1)\n",
    "\n",
    "    data = data[['pm25','pm25_10th']]\n",
    "    data.to_csv('cached_tables/train_air_quality.csv', index=False)\n",
    "\n",
    "\n",
    "    data = pd.read_csv('Submission_template.csv')\n",
    "\n",
    "    data['pm25'] = data.apply(get_pm25, axis=1)\n",
    "    data['pm25_10th'] = data.apply(get_pm25_10th, axis=1)\n",
    "    data = data[['pm25','pm25_10th']]\n",
    "    data.to_csv('cached_tables/val_air_quality.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_data = pd.read_csv('cached_tables/train_air_quality.csv')\n",
    "air_quality_columns = ['pm25','pm25_10th']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (external data) 10. Facilities in NYC ###\n",
    "\n",
    "#### Data page: https://data.cityofnewyork.us/City-Government/Facilities-Database/ji82-xba5/about_data\n",
    "\n",
    "#### Download: \n",
    "In https://data.cityofnewyork.us/City-Government/Facilities-Database/ji82-xba5/about_data, download the data by clicking the \"Export\" button. Save it as raw_data/nyc_facility.csv.\n",
    "\n",
    "#### Data description of nyc_facility.csv:\n",
    "The data contains the facilities in NYC, including the facility name, address, and the facility group. We only use the \"PARKS AND PLAZAS\" facility column\n",
    "\n",
    "#### Feature derivation:\n",
    "for each interested point, we build buffer with radius 0.01 and count the number of facilities in the buffer.\n",
    "1. PARKS AND PLAZAS_count@0.01\n",
    "\n",
    "the computation is done under projection: EPSG:4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "if not os.path.exists('cached_tables/train_facility_features.csv'):\n",
    "\n",
    "    facgroups = ['PARKS AND PLAZAS']\n",
    "    def get_facility_data(lon, lat, df, radius=0.001):\n",
    "        rows = df[abs(df['longitude'] - lon)<radius]\n",
    "        rows = rows[abs(rows['latitude'] - lat)<radius]\n",
    "        features = {}\n",
    "\n",
    "        for facgroup in facgroups:\n",
    "            facgroup_count = len([1 for fg in rows['facgroup'] if fg==facgroup])\n",
    "            features[f'{facgroup}_count@{radius}'] = facgroup_count\n",
    "    \n",
    "        return rows, features\n",
    "\n",
    "    facdb = pd.read_csv('./raw_data/nyc_facility.csv')\n",
    "    facdb = facdb.drop(columns=['uid', 'addressnum', 'facname', 'streetname',\t'address','bbl','cd','council','opabbrev','opname',\t'city','bin','xcoord','ycoord',\t'boro',\t'borocode',\t'zipcode','geometry'])\n",
    "\n",
    "\n",
    "## train\n",
    "    data = pd.read_csv('Training_data_uhi_index.csv')\n",
    "    lons = data['Longitude'].values\n",
    "    lats = data['Latitude'].values\n",
    "\n",
    "    all_features = []\n",
    "\n",
    "    for lon, lat in tqdm(zip(lons,lats),total=len(lons)):\n",
    "        featuresall = {}\n",
    "        for r in [0.01]:\n",
    "            _, features = get_facility_data(lon, lat, facdb, radius=r)\n",
    "            featuresall.update(features)\n",
    "        all_features.append(featuresall)\n",
    "    all_features = pd.DataFrame(all_features)\n",
    "    all_features.to_csv('cached_tables/train_facility_features.csv', index=False)\n",
    "\n",
    "### val\n",
    "    data = pd.read_csv('Submission_template.csv')\n",
    "    lons = data['Longitude'].values\n",
    "    lats = data['Latitude'].values\n",
    "\n",
    "    all_features = []\n",
    "    for lon, lat in tqdm(zip(lons,lats),total=len(lons)):\n",
    "        featuresall = {}\n",
    "        for r in [0.01]:\n",
    "            _, features = get_facility_data(lon, lat, facdb, radius=r)\n",
    "            featuresall.update(features)\n",
    "        all_features.append(featuresall)\n",
    "    all_features = pd.DataFrame(all_features)\n",
    "    all_features.to_csv('cached_tables/val_facility_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_data = pd.read_csv('cached_tables/train_facility_features.csv')\n",
    "facility_columns =  ['PARKS AND PLAZAS_count@0.01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data merging, preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "uhi_data = combine_two_datasets(ground_df, weather_data)\n",
    "# 2.\n",
    "uhi_data = combine_two_datasets(uhi_data, building_footprint)\n",
    "uhi_data = combine_two_datasets(uhi_data, building_footprint_area)\n",
    "# 3.\n",
    "uhi_data = combine_two_datasets(uhi_data, building_elevation_data)\n",
    "# 4.\n",
    "uhi_data = combine_two_datasets(uhi_data, building_far_data)\n",
    "# 5.\n",
    "uhi_data = combine_two_datasets(uhi_data, building_energy_data)\n",
    "# 6.\n",
    "uhi_data = combine_two_datasets(uhi_data, pop_data)\n",
    "# 7.\n",
    "uhi_data = combine_two_datasets(uhi_data, inequality_data)\n",
    "# 8.\n",
    "uhi_data = combine_two_datasets(uhi_data, commute_data)\n",
    "# 9.\n",
    "uhi_data = combine_two_datasets(uhi_data, air_quality_data)\n",
    "# 10.\n",
    "uhi_data = combine_two_datasets(uhi_data, facility_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check =  weather_columns + building_footprint_columns + building_footprint_area_columns + building_elevation_columns + building_far_columns + building_energy_columns + pop_columns + inequality_columns + commute_columns + air_quality_columns + facility_columns\n",
    "for col in columns_to_check:\n",
    "    uhi_data[col] = uhi_data[col].apply(lambda x: tuple(x) if isinstance(x, np.ndarray) and x.ndim > 0 else x)\n",
    "uhi_data.drop(columns=['Longitude','Latitude','datetime'], inplace=True)\n",
    "mean_values = uhi_data.mean()\n",
    "uhi_data = uhi_data.fillna(mean_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:01<00:00, 44.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11229, 1936)\n",
      "(11229,)\n",
      "r2_train: 1.0, r2_test: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "selected_columns = selected_columns = [\n",
    "                        'w1',\n",
    "                        'num_buildings_0.06', \n",
    "                        'num_buildings_0.07', \n",
    "                        'num_buildings_0.08',\n",
    "                        'num_buildings_0.09', \n",
    "                        'num_buildings_0.1',\n",
    "                        'building_area_0.0005',\n",
    "                        'building_area_0.001',\n",
    "                        'building_area_0.002',\n",
    "                        'building_area_0.003',\n",
    "                        'building_area_0.004',\n",
    "                        'building_area_0.005',  \n",
    "                        'building_area_0.006',\n",
    "                        'building_area_0.007',\n",
    "                        'building_area_0.008',\n",
    "                        'building_area_0.009',\n",
    "                        'building_area_0.01',\n",
    "                        'building_area_0.02',\n",
    "                        'building_area_0.03', \n",
    "                        'building_area_0.04', \n",
    "                        'building_area_0.05', \n",
    "                        'building_area_0.06', \n",
    "                        'building_area_0.07', \n",
    "                        'building_area_0.08',\n",
    "                        'building_area_0.09',\n",
    "                        'building_area_0.1',\n",
    "\n",
    "                        'building_z_grade_0.005',\n",
    "                        'building_z_floor_0.005',\n",
    "                        'building_z_grade_0.01',\n",
    "                        'building_z_floor_0.01',\n",
    "\n",
    "                        'building_eui_0.005',\n",
    "                        'building_eui_0.01',\n",
    "                        'building_eui_0.15',\n",
    "                        \n",
    "\n",
    "                        'FAR@500',\n",
    "                        'TotalPop', \n",
    "\n",
    "                        'category_id', \n",
    "                        'area', \n",
    "                        'residential' ,\n",
    "                        'commercial', \n",
    "                        'industrial',\n",
    "                        'public_transportation', \n",
    "\n",
    "                        'pm25',\n",
    "                        'pm25_10th',\n",
    "                        'PARKS AND PLAZAS_count@0.01',\n",
    "                        'UHI Index']\n",
    "\n",
    "model = ExtraTreesRegressor(n_estimators=5000, max_depth=None, n_jobs=-1, random_state=142, warm_start=True, max_features='sqrt', criterion='squared_error')\n",
    "\n",
    "selected_columns_new = [c for c in selected_columns]\n",
    "uhi_data = uhi_data[selected_columns]\n",
    "columns = uhi_data.columns.tolist()\n",
    "for col_1 in tqdm(columns):\n",
    "    for col_2 in columns:\n",
    "        if col_1 != 'UHI Index' and col_2 != 'UHI Index' and col_1 != col_2:\n",
    "            uhi_data[f\"{col_1}*{col_2}\"] = uhi_data[col_1] * uhi_data[col_2]\n",
    "            selected_columns_new.append(f\"{col_1}*{col_2}\")\n",
    "\n",
    "uhi_data = uhi_data[selected_columns_new]\n",
    "uhi_data.to_csv('train_crossed.csv', index=False)\n",
    "# fit on full data\n",
    "sc, sc_y, model, r2_train, r2_test = get_perf(uhi_data, model, selected_columns_new, split_ratio=0.)\n",
    "print(f\"r2_train: {r2_train}, r2_test: {r2_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model saving ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "# with open('model.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('Submission_template.csv')\n",
    "val_building_footprint = pd.read_csv('cached_tables/val_building_footprint.csv')\n",
    "val_building_footprint_area = pd.read_csv('cached_tables/val_building_footprint_area.csv')\n",
    "val_building_elevation_data = pd.read_csv('cached_tables/val_building_elevation.csv')\n",
    "val_building_far_data = pd.read_csv('cached_tables/val_building_far.csv')\n",
    "val_building_energy_data = pd.read_csv('cached_tables/val_building_energy.csv')\n",
    "val_pop_data = pd.read_csv('cached_tables/val_pop_data.csv')\n",
    "val_inequality_data = pd.read_csv('cached_tables/val_inequality.csv')\n",
    "val_commute_data = pd.read_csv('cached_tables/val_commute.csv')\n",
    "val_air_quality_data = pd.read_csv('cached_tables/val_air_quality.csv')\n",
    "val_facility_data = pd.read_csv('cached_tables/val_facility_features.csv')\n",
    "\n",
    "# 1.\n",
    "val_data, _ = define_area_and_weather(test_df)\n",
    "# 2.\n",
    "val_data = combine_two_datasets(val_data, val_building_footprint)\n",
    "val_data = combine_two_datasets(val_data, val_building_footprint_area)\n",
    "# 3.\n",
    "val_data = combine_two_datasets(val_data, val_building_elevation_data)\n",
    "# 4.\n",
    "val_data = combine_two_datasets(val_data, val_building_far_data)\n",
    "# 5.\n",
    "val_data = combine_two_datasets(val_data, val_building_energy_data)\n",
    "# 6.\n",
    "val_data = combine_two_datasets(val_data, val_pop_data)\n",
    "# 7.\n",
    "val_data = combine_two_datasets(val_data, val_inequality_data)\n",
    "# 8.\n",
    "val_data = combine_two_datasets(val_data, val_commute_data)\n",
    "# 9.\n",
    "val_data = combine_two_datasets(val_data, val_air_quality_data)\n",
    "# 10.\n",
    "val_data = combine_two_datasets(val_data, val_facility_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_to_check:\n",
    "    val_data[col] = val_data[col].apply(lambda x: tuple(x) if isinstance(x, np.ndarray) and x.ndim > 0 else x)\n",
    "selected_columns.remove('UHI Index')\n",
    "selected_columns_new.remove('UHI Index')\n",
    "val_data = val_data[selected_columns]\n",
    "val_data = val_data.fillna(mean_values)\n",
    "columns = val_data.columns.tolist()\n",
    "for col_1 in columns:\n",
    "    for col_2 in columns:\n",
    "        if col_1 != 'UHI Index' and col_2 != 'UHI Index' and col_1 != col_2:\n",
    "            val_data[f\"{col_1}*{col_2}\"] = val_data[col_1] * val_data[col_2]\n",
    "\n",
    "val_data.to_csv('val_crossed.csv', index=False)\n",
    "transformed_val_data = sc.transform(val_data)\n",
    "\n",
    "#Making predictions\n",
    "final_predictions = model.predict(transformed_val_data)\n",
    "\n",
    "final_predictions = sc_y.inverse_transform(final_predictions.reshape(-1, 1)).flatten()\n",
    "final_prediction_series = pd.Series(final_predictions)\n",
    "final_prediction_series = round(final_prediction_series, 5)\n",
    "\n",
    "#Combining the results into dataframe\n",
    "submission_df = pd.DataFrame({'Longitude':test_df['Longitude'].values, 'Latitude':test_df['Latitude'].values, 'UHI Index':final_prediction_series.values})\n",
    "\n",
    "#Displaying the sample submission dataframe\n",
    "submission_df.head()\n",
    "\n",
    "submission_df.to_csv(f\"Submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
